input:
  label: "cron"
  generate:
    mapping: |
      root = {
        "query":"query RegisteredGuilds { registeredGuilds { guildId } }"
      }

    interval: "0 15,45 * * * *"

pipeline:
  processors:
    - label: "log_start_message"
      log:
        level: INFO
        message: "Processing message..."

    - label: "get_registered_guilds"
      http:
        url: ${GRAPHQL_URL}
        verb: POST
        rate_limit: ""
        timeout: 10s
        parallel: false
        headers:
          Content-Type: application/json

    - label: "deleted_errored_gql"
      bloblang: root = if errored() { deleted() }

    - label: "extract_guild_id"
      bloblang: |
        root = this.data.registeredGuilds.map_each(g -> g.guildId)

    - label: "unarchive_guild_ids"
      unarchive:
        format: json_array

    - label: "set_guild_id_meta"
      bloblang: |
        meta guildId = content().string().unquote()

    - label: "get_cache"
      branch:
        processors:
          - cache:
              resource: event_listener_cache
              operator: get
              key: 'last_event_${! meta("guildId") }'
        result_map: |
          meta last_event = content().number()

    - label: "handle_get_cache_error"
      catch:
        - bloblang: |
            root = this
            meta last_event = 0

    - label: "initialize_ao_api_loop"
      bloblang: |
        meta offset = 0
        meta current_event = 0
        meta limit = 25

    - label: call_ao_api
      while:
        at_least_once: true
        check: meta("current_event") == 0 || meta("current_event").number() > meta("last_event").number()
        max_loops: 5
        processors:
          - branch:
              processors:
                - http:
                    url: >-
                      ${EVENTS_API_URL}?limit=${! meta("limit") }&offset=${! meta("offset") }&guildId=${! meta("guildId").string() }
                    verb: GET
                    rate_limit: event_searches
                    timeout: 10s
                    parallel: false
                    headers:
                      Content-Type: application/json

                - bloblang: root = if errored() { deleted() }

              result_map: |
                let lim = meta("limit").number()
                meta offset = meta("offset").number() + $lim
                meta current_event = this.index(this.length() - 1).EventId
                root.results = (root.results | []).append(this)

    - label: "flatten_results"
      bloblang: root.Events = this.results.flatten()

    - label: events_processor
      bloblang: |

        import "./src/mappers/event_mapper.blobl"

        root.Events = this.apply("map_events")
        meta last_event = ( root.Events.0.EventId | meta("last_event") )
        meta processed = root.Events.length()

    - label: "set_cache"
      cache:
        resource: event_listener_cache
        operator: set
        key: 'last_event_${! meta("guildId") }'
        value: '${! meta("last_event").number() }'

    - label: "log_proccessed_count"
      log:
        level: INFO
        message: 'records_processed for ${! meta("guildId") }: ${! meta("processed") }'

    - label: "sort_events"
      bloblang: root = this.Events.sort_by(ele -> ele.EventId)

    - label: "unarchive_events"
      unarchive:
        format: json_array

    - label: "check_unarchive_errors"
      bloblang: root = if errored() { deleted() }

output:
  # stdout: {}
  kafka:
    addresses:
      - broker:29092
    topic: ao_events
    key: ${! json("EventId") }

cache_resources:
  - label: event_listener_cache
    redis:
      url: redis://ao_cache:6379

rate_limit_resources:
  - label: event_searches
    local:
      count: 30
      interval: 30m
